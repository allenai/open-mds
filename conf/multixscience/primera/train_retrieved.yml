# ModelArguments
model_name_or_path: "allenai/PRIMERA-multixscience"

# DataTrainingArguments
dataset_name: "allenai/multixscience_dense_mean"
text_column: "abstract"
summary_column: "related_work"
max_source_length: 4096
max_target_length: 1024

# Seq2SeqTrainingArguments
# See: https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments
do_train: true
per_device_train_batch_size: 1
per_device_eval_batch_size: 12
# per_device_train_batch_size = 1 * gradient_accumulation_steps = 16 = total effective batch size of 16
gradient_accumulation_steps: 16
learning_rate: 3e-5
num_train_epochs: 3
warmup_ratio: 0.10
# Controls the checkpointing strategy
save_strategy: "steps"
# 30369 examples * 3 epochs / batch size of 16 = 8432 steps. For 12 evaluations we need 8432 / 12 = 474 steps
save_steps: 474
save_total_limit: 12