# ModelArguments
model_name_or_path: "allenai/led-base-16384"

# DataTrainingArguments
dataset_name: "allenai/mslr2022"
dataset_config_name: "ms2"
text_column: "background"
summary_column: "target"
max_source_length: 16384
max_target_length: 256

# Seq2SeqTrainingArguments
do_train: true
do_eval: true
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 4  # Expects 4 GPUs to be used for a total effective batch size of 16
learning_rate: 3e-5
weight_decay: 0.01
num_train_epochs: 20
warmup_ratio: 0.1
label_smoothing_factor: 0.1
# Controls the evaluation strategy
evaluation_strategy: "steps"
eval_steps: 500
eval_delay: 5000
# Controls the checkpointing strategy
save_strategy: "steps"
save_steps: 500
save_total_limit: 1
load_best_model_at_end: true
metric_for_best_model: "bertscore_f1_mean"