{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting\n",
    "\n",
    "This notebook organizes the analysis of the sorting experiments. Primarily, it just runs t-tests comparing the sorting performance to the baseline performance and reports any significant differences.\n",
    "\n",
    "Run the following cells to import the required packages and load some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import scipy.stats as stats\n",
    "from retrieval_exploration.common import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point the variable `data_dir` to the location of a directory that contains the results of running the [`run_summarization.py`](../scripts/run_summarization.py) script for one or more models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mcochrane\u001b[m\u001b[m      \u001b[1m\u001b[36mms2\u001b[m\u001b[m           \u001b[1m\u001b[36mmultinews\u001b[m\u001b[m     \u001b[1m\u001b[36mmultixscience\u001b[m\u001b[m \u001b[1m\u001b[36mwcep\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../output/results/\"\n",
    "# Make sure the directory exists and contains the expected subdirectories\n",
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the following block to perform the significance tests. Any significant differences will be reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:24<00:00,  1.99it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 48/48 [01:00<00:00,  1.27s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.30s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.21s/it]\n",
      "100%|██████████| 48/48 [04:19<00:00,  5.41s/it]\n",
      "100%|██████████| 48/48 [01:39<00:00,  2.07s/it]\n",
      "100%|██████████| 48/48 [03:19<00:00,  4.16s/it]\n",
      "100%|██████████| 48/48 [01:49<00:00,  2.28s/it]\n",
      "0it [00:00, ?it/s]\n",
      "/var/folders/g5/4f6qx6cd5nngsv1qkqv541wr0000gn/T/ipykernel_25045/4578664.py:26: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  perturbed_scores = perturbed_df[perturbed_df.model_name_or_path == model_name_or_path][\n"
     ]
    }
   ],
   "source": [
    "for subdir in Path(data_dir).iterdir():\n",
    "    # Some datasets have blind test splits, and so we evaluate on the validation set\n",
    "    # HuggingFace assigns a different prefix to the keys in the output json, so set that here\n",
    "    metric_key_prefix = \"eval\" if subdir.name in {\"ms2\", \"cochrane\"} else \"predict\"\n",
    "\n",
    "    # The metrics we want to plot the delta for\n",
    "    metric_columns = [\n",
    "        f\"{metric_key_prefix}_rouge_avg_fmeasure\",\n",
    "        f\"{metric_key_prefix}_bertscore_f1\",\n",
    "    ]\n",
    "    # Load the results as dataframes\n",
    "    baseline_df, perturbed_df = util.load_results_dicts(\n",
    "        data_dir=subdir,\n",
    "        metric_columns=metric_columns,\n",
    "        metric_key_prefix=metric_key_prefix,\n",
    "    )\n",
    "\n",
    "    # Only care about sorting results\n",
    "    perturbed_df = perturbed_df[perturbed_df[f\"{metric_key_prefix}_perturbation\"] == \"sorting\"]\n",
    "\n",
    "    # Perform the signifiance test for all models, selection strategies, and metrics\n",
    "    for model_name_or_path in perturbed_df.model_name_or_path.unique():\n",
    "        for selection_strategy in perturbed_df[f\"{metric_key_prefix}_selection_strategy\"].unique():\n",
    "            for metric in metric_columns:\n",
    "                baseline_scores = baseline_df[baseline_df.model_name_or_path == model_name_or_path][metric]\n",
    "                perturbed_scores = perturbed_df[perturbed_df.model_name_or_path == model_name_or_path][\n",
    "                    perturbed_df[f\"{metric_key_prefix}_selection_strategy\"] == selection_strategy\n",
    "                ][metric]\n",
    "\n",
    "                _, pvalue = stats.ttest_ind(baseline_scores, perturbed_scores)\n",
    "\n",
    "                if pvalue < 0.05:\n",
    "                    print(\n",
    "                        f\"Model {model_name_or_path} with selection strategy {selection_strategy} has a\"\n",
    "                        f\" significant difference in {metric} with p-value {pvalue}.\"\n",
    "                        f\" Baseline: {baseline_scores.mean()}, Perturbed: {perturbed_scores.mean()}\"\n",
    "                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('retrieval-exploration-C-nHY9Y1-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf84cdef7bef6c8a571de4ac52ff8ec016cbb22b7bda79a5961c2e1a10655180"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
