{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting Perturbation Experiment\n",
    "\n",
    "This notebook organizes the analysis of the sorting perturbation experiment.\n",
    "\n",
    "__Note__: if you are running this notebook in colab, uncomment and run the following cell to install the project and its dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"git+https://github.com/allenai/open-mds.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to import the required packages and load some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "from open_mds.common import util\n",
    "\n",
    "# Threshold under which to reject the null hypothesis\n",
    "THRESHOLD = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point the variable `data_dir` to the location of a directory that contains the results of running the [`run_summarization.py`](../scripts/run_summarization.py) script for one or more models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../output/results/\"\n",
    "# Make sure the directory exists and contains the expected subdirectories\n",
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the following cell to produce the tabulated results and run the significance tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"dataset\": [],\n",
    "    \"model\": [],\n",
    "    \"selection_strategy\": [],\n",
    "    \"metric\": [],\n",
    "    \"baseline\": [],\n",
    "    \"perturbed\": [],\n",
    "    \"difference\": [],\n",
    "    \"significant\": [],\n",
    "}\n",
    "\n",
    "for subdir in Path(data_dir).iterdir():\n",
    "    # Some datasets have blind test splits, and so we evaluate on the validation set\n",
    "    # HuggingFace assigns a different prefix to the keys in the output json, so set that here\n",
    "    metric_key_prefix = \"eval\" if subdir.name in {\"ms2\", \"cochrane\"} else \"predict\"\n",
    "\n",
    "    # The metrics we want to check for significance\n",
    "    metric_columns = [\n",
    "        f\"{metric_key_prefix}_rouge_avg_fmeasure\",\n",
    "        f\"{metric_key_prefix}_bertscore_f1\",\n",
    "    ]\n",
    "    # Load the results as dataframes\n",
    "    baseline_df, perturbed_df = util.load_results_dicts(\n",
    "        data_dir=subdir,\n",
    "        metric_columns=metric_columns,\n",
    "        metric_key_prefix=metric_key_prefix,\n",
    "        # Only retain data that pertains to the perturbation experiments\n",
    "        load_retrieval_results=False,\n",
    "    )\n",
    "\n",
    "    # We only care about sorting results\n",
    "    perturbed_df = perturbed_df[perturbed_df[f\"{metric_key_prefix}_perturbation\"] == \"sorting\"]\n",
    "\n",
    "    # Perform the signifiance test for all models, selection strategies, and metrics\n",
    "    for model_name_or_path in perturbed_df.model_name_or_path.unique():\n",
    "        for selection_strategy in [\"random\", \"oracle\"]:\n",
    "            for metric in metric_columns:\n",
    "                # Isolate the results from one experiment\n",
    "                experiment_df = perturbed_df[perturbed_df.model_name_or_path == model_name_or_path][\n",
    "                    perturbed_df[f\"{metric_key_prefix}_selection_strategy\"] == selection_strategy\n",
    "                ]\n",
    "\n",
    "                baseline_scores = baseline_df[baseline_df.model_name_or_path == model_name_or_path][metric]\n",
    "                perturbed_scores = experiment_df[metric]\n",
    "                perturbed_scores_delta = experiment_df[f\"{metric}_delta\"]\n",
    "\n",
    "                # Report any significant differences\n",
    "                _, pvalue = stats.ttest_rel(baseline_scores, perturbed_scores)\n",
    "                if pvalue < THRESHOLD:\n",
    "                    print(\n",
    "                        f\"Model {model_name_or_path} with selection strategy {selection_strategy} has a\"\n",
    "                        f\" significant difference in {metric} with p-value {pvalue}.\"\n",
    "                        f\" Baseline: {baseline_scores.mean()}, Perturbed: {perturbed_scores.mean()}\"\n",
    "                    )\n",
    "\n",
    "                # Collect the results we are interested in\n",
    "                metric_key = metric.removeprefix(f\"{metric_key_prefix}_\")\n",
    "                results[\"dataset\"].append(subdir.name)\n",
    "                results[\"model\"].append(model_name_or_path)\n",
    "                results[\"selection_strategy\"].append(selection_strategy)\n",
    "                results[\"metric\"].append(metric_key)\n",
    "                results[\"baseline\"].append(round(baseline_scores.mean(), 2))\n",
    "                results[\"perturbed\"].append(round(perturbed_scores.mean(), 2))\n",
    "                results[\"difference\"].append(round(perturbed_scores_delta.mean(), 2))\n",
    "                results[\"significant\"].append(pvalue < THRESHOLD)\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wish to subset the results dataframe by dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df.dataset == \"multinews\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('open-mds-KCQg3FFx-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1de8a9ac52ca6a8e43f03f98931f021acf0adbba0952f200363bc18db767aa0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
