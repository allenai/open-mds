{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from retrieval_exploration.common import util\n",
    "\n",
    "# Use the same styling for all plots & figures in the paper\n",
    "sns.set_theme(context=\"paper\", style=\"ticks\", palette=\"tab10\", font_scale=1.375)\n",
    "\n",
    "# Display all rows/columns in DataFrame\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../output/results\"\n",
    "# Make sure the directory exists and contains the expected subdirectories\n",
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dfs = []\n",
    "pretty_ds_names = {\n",
    "    \"multinews\": \"Multi-News\",\n",
    "    \"wcep\": \"WCEP-10\",\n",
    "    \"multixscience\": \"Multi-XScience\",\n",
    "    \"ms2\": \"MS^2\",\n",
    "    \"cochrane\": \"Cochrane\",\n",
    "}\n",
    "\n",
    "for subdir in Path(data_dir).iterdir():\n",
    "    if subdir.name in [\"multinews\", \"multixscience\"]:\n",
    "        include_models = [\"primera\"]\n",
    "    elif subdir.name == \"wcep\":\n",
    "        include_models = [\"lsg-bart-base\"]\n",
    "    else:\n",
    "        include_models = [\"led-base\"]\n",
    "\n",
    "    # Here, we collect the data for \"checkpoint 0\". For the gold evaluation, this is simply the baseline model results.\n",
    "    # For the retrieved evaluation, this is the results of whatever retriever/top-k strategy we trained on, which will\n",
    "    # was a dense retriever with mean top-k strategy in the paper.\n",
    "    for model_dir in subdir.iterdir():\n",
    "        if model_dir.name not in include_models:\n",
    "            continue\n",
    "        gold_checkpoint_0 = model_dir / util._TRAINING_DIR / \"gold\" / \"checkpoint-0\"\n",
    "        retrieved_checkpoint_0 = model_dir / util._TRAINING_DIR / \"retrieved\" / \"checkpoint-0\"\n",
    "        gold_checkpoint_0.mkdir(parents=True, exist_ok=True)\n",
    "        retrieved_checkpoint_0.mkdir(parents=True, exist_ok=True)\n",
    "        gold_baseline = model_dir / util._BASELINE_DIR / util._RESULTS_FILENAME\n",
    "        retrieved_baseline = model_dir / util._RETRIEVAL_DIR / \"dense\" / \"mean\" / util._RESULTS_FILENAME\n",
    "        !cp $gold_baseline $gold_checkpoint_0\n",
    "        !cp $retrieved_baseline $retrieved_checkpoint_0\n",
    "\n",
    "    # Some datasets have blind test splits, and so we evaluate on the validation set\n",
    "    # HuggingFace assigns a different prefix to the keys in the output json, so set that here\n",
    "    metric_key_prefix = \"eval\" if subdir.name in {\"ms2\", \"cochrane\"} else \"predict\"\n",
    "\n",
    "    # The metrics we want to record results for\n",
    "    metric_columns = [\n",
    "        f\"{metric_key_prefix}_rouge_avg_fmeasure\",\n",
    "        f\"{metric_key_prefix}_bertscore_f1\",\n",
    "    ]\n",
    "\n",
    "    # Load the results as dataframes\n",
    "    _, training_df = util.load_results_dicts(\n",
    "        data_dir=subdir,\n",
    "        include_models=include_models,\n",
    "        metric_columns=metric_columns,\n",
    "        metric_key_prefix=metric_key_prefix,\n",
    "        # Only retain data that pertains to the training experiments\n",
    "        load_perturbation_results=False,\n",
    "        load_retrieval_results=False,\n",
    "        load_training_results=True,\n",
    "    )\n",
    "\n",
    "    # Add the name of the dataset and model\n",
    "    training_df[\"dataset_name\"] = [pretty_ds_names[subdir.name]] * len(training_df)\n",
    "    training_df.model_name_or_path = [include_models[0]] * len(training_df)\n",
    "    # Remove the metric key prefix to plot everything with the same label\n",
    "    training_df[\"rouge_avg_fmeasure_delta\"] = training_df[f\"{metric_key_prefix}_rouge_avg_fmeasure_delta\"]\n",
    "    training_df[\"bertscore_f1_delta\"] = training_df[f\"{metric_key_prefix}_bertscore_f1_delta\"]\n",
    "    # Deduce the number of steps from the checkpoint name\n",
    "    training_df[\"frac_additional_steps\"] = training_df.checkpoint / training_df.checkpoint.max() * 3\n",
    "    # Specify the evaluation type\n",
    "    training_df[\"evaluation\"] = [\n",
    "        \"retrieved\" if not retriever else \"gold\"\n",
    "        for retriever in training_df[f\"{metric_key_prefix}_retriever\"].isnull()\n",
    "    ]\n",
    "\n",
    "    training_dfs.append(training_df[training_df.evaluation == \"retrieved\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(training_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(4.2, 6.4))\n",
    "\n",
    "for i, metric in enumerate([\"rouge_avg_fmeasure_delta\", \"bertscore_f1_delta\"]):\n",
    "\n",
    "    g = sns.lineplot(\n",
    "        data=df,\n",
    "        x=\"frac_additional_steps\",\n",
    "        y=metric,\n",
    "        style=\"dataset_name\",\n",
    "        hue=\"dataset_name\",\n",
    "        hue_order=pretty_ds_names.values(),\n",
    "        ci=None,\n",
    "        markers=True,\n",
    "        ax=axes[i],\n",
    "    )\n",
    "    g.set(\n",
    "        xlabel=\"Fraction of additional training steps\" if i == 1 else None,\n",
    "        ylabel=\"Δ ROUGE-Avg F1\" if metric == \"rouge_avg_fmeasure_delta\" else \"Δ BERTScore F1\",\n",
    "    )\n",
    "\n",
    "# Additional per-axis styling\n",
    "for ax in axes.flatten():\n",
    "    # Add yaxis grid\n",
    "    ax.yaxis.grid(True)\n",
    "    ax.tick_params(left=False)\n",
    "\n",
    "\n",
    "axes[1].legend_ = None\n",
    "# handles, labels = axes[0].get_legend_handles_labels()\n",
    "# axes[0].legend(handles=handles[:], labels=labels[:])\n",
    "# axes[0].legend(loc=\"lower center\")\n",
    "\n",
    "axes[0].legend(\n",
    "    loc=\"lower right\",\n",
    "    bbox_to_anchor=(1.0, 1.0),\n",
    "    ncol=3,\n",
    "    frameon=False,\n",
    "    handletextpad=0.325,\n",
    "    columnspacing=0.325,\n",
    "    fontsize=\"small\",\n",
    ")\n",
    "\n",
    "# Additional global styling\n",
    "sns.despine(left=True)\n",
    "\n",
    "AVG_ROUGE_IMPROVEMENT=0.49\n",
    "# Optionally, shade the ROUGE plots to better contextualize the results\n",
    "y_min, y_max = axes[0].get_ylim()\n",
    "axes[0].fill_between(\n",
    "    list(range(4)), y_max, -AVG_ROUGE_IMPROVEMENT, alpha=0.0875, facecolor=\"green\"\n",
    ")\n",
    "axes[0].fill_between(list(range(4)), -AVG_ROUGE_IMPROVEMENT, y_min, alpha=0.04, facecolor=\"red\")\n",
    "\n",
    "\n",
    "plt.subplots_adjust(top=0.90, hspace=0.1)\n",
    "plt.savefig(f\"../output/plots/training.svg\", facecolor=\"white\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('retrieval-exploration-C-nHY9Y1-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf84cdef7bef6c8a571de4ac52ff8ec016cbb22b7bda79a5961c2e1a10655180"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
