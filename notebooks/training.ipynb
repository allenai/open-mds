{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Experiments\n",
    "\n",
    "This notebook organizes the analysis of the experiments which fine-tune existing summarizers in the open-domain setting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to import the required packages and load some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "from open_mds.common import util\n",
    "\n",
    "# Use the same styling for all plots & figures in the paper\n",
    "sns.set_theme(context=\"paper\", style=\"ticks\", palette=\"tab10\", font_scale=1.375)\n",
    "\n",
    "# Display all rows/columns in DataFrame\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# This is the average improvement in summarization performance in *ACL conferences. We will use it to shade our\n",
    "# figures to better contextualize the results of our experiments.\n",
    "# See: https://aclanthology.org/2022.naacl-main.442/\n",
    "AVG_ROUGE_IMPROVEMENT = 0.49\n",
    "\n",
    "# Colors from our palette that we need to access directly (e.g. for shading the figure)\n",
    "GREEN = \"#2ca02c\"\n",
    "RED = \"#d62728\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_plot(\n",
    "    axes,\n",
    "    include_shading: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Style the resulting plots for the training experiments.\"\"\"\n",
    "\n",
    "    # Set axis labels\n",
    "    axes[0][0].set_title(\"retrieved\")\n",
    "    axes[0][1].set_title(\"ground-truth\")\n",
    "    axes[1][0].set_xlabel(\"additional training epochs\")\n",
    "    axes[0][0].set_ylabel(\"Δ ROUGE-Avg F1\")\n",
    "    axes[1][0].set_ylabel(\"Δ BERTScore F1\")\n",
    "\n",
    "    # Additional per-axis styling\n",
    "    for ax in axes.flatten():\n",
    "        # Add yaxis grid\n",
    "        ax.yaxis.grid(True, zorder=-1)\n",
    "        ax.tick_params(left=False)\n",
    "\n",
    "        # Add a horizontal line at 0. It will be behind the data but in front of the grid lines.\n",
    "        ax.axhline(y=0, color=\"#7f7f7f\", linestyle=\"--\", linewidth=1.5, zorder=1)\n",
    "\n",
    "    # Remove the x tick labels from bottom right plot\n",
    "    axes[1][1].tick_params(labelbottom=False)\n",
    "\n",
    "    # Place one legend at the top of the plot\n",
    "    for ax in axes.flatten():\n",
    "        ax.legend_ = None\n",
    "        # Minor ticks every 0.5 epochs\n",
    "        ax.xaxis.set_minor_locator(MultipleLocator(0.5))\n",
    "\n",
    "    axes[0][1].legend(\n",
    "        loc=\"lower right\",\n",
    "        bbox_to_anchor=(1.03, 1.15),\n",
    "        ncol=5,\n",
    "        frameon=False,\n",
    "        handletextpad=0.325,\n",
    "        columnspacing=0.325,\n",
    "        fontsize=\"small\",\n",
    "    )\n",
    "\n",
    "    if include_shading:\n",
    "        # Optionally, shade the ROUGE plots to better contextualize the results\n",
    "        y_min = min([ax.get_ylim()[0] for ax in axes[0]])\n",
    "        y_max = max([ax.get_ylim()[1] for ax in axes[0]])\n",
    "        for ax in axes[0]:\n",
    "            ax.fill_between(list(range(4)), y_max, -AVG_ROUGE_IMPROVEMENT, alpha=0.1, facecolor=GREEN)\n",
    "            ax.fill_between(list(range(4)), -AVG_ROUGE_IMPROVEMENT, y_min, alpha=0.06, facecolor=RED)\n",
    "\n",
    "    # Additional global styling\n",
    "    sns.despine(left=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point the variable `data_dir` to the location of a directory that contains the results of running the [`run_summarization.py`](../scripts/run_summarization.py) script for one or more models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../output/results\"\n",
    "# Make sure the directory exists and contains the expected subdirectories\n",
    "!ls $data_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, you may also set a couple flags to control modifications to the plots that improve visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shade differences greater than the average yearly improvement in summarization performance (ROUGE only)\n",
    "include_shading = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dfs = []\n",
    "pretty_ds_names = {\n",
    "    \"multinews\": \"Multi-News\",\n",
    "    \"wcep\": \"WCEP-10\",\n",
    "    \"multixscience\": \"Multi-XScience\",\n",
    "    \"ms2\": \"MS^2\",\n",
    "    \"cochrane\": \"Cochrane\",\n",
    "}\n",
    "\n",
    "# Only load the best model for each dataset\n",
    "for subdir in Path(data_dir).iterdir():\n",
    "    if subdir.name in [\"multinews\", \"multixscience\"]:\n",
    "        include_models = [\"primera\"]\n",
    "    elif subdir.name == \"wcep\":\n",
    "        include_models = [\"lsg-bart-base\"]\n",
    "    else:\n",
    "        include_models = [\"led-base\"]\n",
    "\n",
    "    # Here, we collect the data for \"checkpoint 0\". For the gold evaluation, this is simply the baseline model results.\n",
    "    # For the retrieved evaluation, this is the results of whatever retriever/top-k strategy we trained on, which will\n",
    "    # was a dense retriever with mean top-k strategy in the paper.\n",
    "    for model_dir in subdir.iterdir():\n",
    "        if model_dir.name not in include_models:\n",
    "            continue\n",
    "        gold_checkpoint_0 = model_dir / util._TRAINING_DIR / \"gold\" / \"checkpoint-0\"\n",
    "        retrieved_checkpoint_0 = model_dir / util._TRAINING_DIR / \"retrieved\" / \"checkpoint-0\"\n",
    "        gold_checkpoint_0.mkdir(parents=True, exist_ok=True)\n",
    "        retrieved_checkpoint_0.mkdir(parents=True, exist_ok=True)\n",
    "        gold_baseline = model_dir / util._BASELINE_DIR / util._RESULTS_FILENAME\n",
    "        retrieved_baseline = model_dir / util._RETRIEVAL_DIR / \"dense\" / \"mean\" / util._RESULTS_FILENAME\n",
    "        !cp $gold_baseline $gold_checkpoint_0\n",
    "        !cp $retrieved_baseline $retrieved_checkpoint_0\n",
    "\n",
    "    # Some datasets have blind test splits, and so we evaluate on the validation set\n",
    "    # HuggingFace assigns a different prefix to the keys in the output json, so set that here\n",
    "    metric_key_prefix = \"eval\" if subdir.name in {\"ms2\", \"cochrane\"} else \"predict\"\n",
    "\n",
    "    # The metrics we want to record results for\n",
    "    metric_columns = [\n",
    "        f\"{metric_key_prefix}_rouge_avg_fmeasure\",\n",
    "        f\"{metric_key_prefix}_bertscore_f1\",\n",
    "    ]\n",
    "\n",
    "    # Load the results as dataframes\n",
    "    _, training_df = util.load_results_dicts(\n",
    "        data_dir=subdir,\n",
    "        include_models=include_models,\n",
    "        metric_columns=metric_columns,\n",
    "        metric_key_prefix=metric_key_prefix,\n",
    "        # Only retain data that pertains to the training experiments\n",
    "        load_perturbation_results=False,\n",
    "        load_retrieval_results=False,\n",
    "        load_training_results=True,\n",
    "    )\n",
    "\n",
    "    # Add the name of the dataset and model\n",
    "    training_df[\"dataset_name\"] = [pretty_ds_names[subdir.name]] * len(training_df)\n",
    "    training_df.model_name_or_path = [include_models[0]] * len(training_df)\n",
    "    # Remove the metric key prefix to plot everything with the same label\n",
    "    training_df[\"rouge_avg_fmeasure_delta\"] = training_df[f\"{metric_key_prefix}_rouge_avg_fmeasure_delta\"]\n",
    "    training_df[\"bertscore_f1_delta\"] = training_df[f\"{metric_key_prefix}_bertscore_f1_delta\"]\n",
    "    # Deduce the number of steps from the checkpoint name\n",
    "    training_df[\"frac_additional_steps\"] = training_df.checkpoint / training_df.checkpoint.max() * 3\n",
    "    # Specify the evaluation type\n",
    "    training_df[\"evaluation\"] = [\n",
    "        \"retrieved\" if not retriever else \"gold\"\n",
    "        for retriever in training_df[f\"{metric_key_prefix}_retriever\"].isnull()\n",
    "    ]\n",
    "\n",
    "    training_dfs.append(training_df)\n",
    "\n",
    "df = pd.concat(training_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=\"row\")\n",
    "\n",
    "for i, metric in enumerate([\"rouge_avg_fmeasure_delta\", \"bertscore_f1_delta\"]):\n",
    "    for j, evaluation in enumerate([\"retrieved\", \"gold\"]):\n",
    "\n",
    "        g = sns.lineplot(\n",
    "            data=df[df.evaluation == evaluation],\n",
    "            x=\"frac_additional_steps\",\n",
    "            y=metric,\n",
    "            style=\"dataset_name\",\n",
    "            hue=\"dataset_name\",\n",
    "            hue_order=pretty_ds_names.values(),\n",
    "            errorbar=None,\n",
    "            markers=True,\n",
    "            ax=axes[i][j],\n",
    "        )\n",
    "        g.set(xlabel=\"\", ylabel=\"\", title=\"\")\n",
    "\n",
    "\n",
    "style_plot(axes, include_shading=include_shading)\n",
    "\n",
    "# Save the figure\n",
    "filename = \"training\"\n",
    "if include_shading:\n",
    "    filename += \"_shaded\"\n",
    "\n",
    "plt.subplots_adjust(top=0.90, wspace=0.0175, hspace=0.075)\n",
    "Path(\"../output/plots\").mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(f\"../output/plots/{filename}.svg\", facecolor=\"white\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('open-mds-KCQg3FFx-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1de8a9ac52ca6a8e43f03f98931f021acf0adbba0952f200363bc18db767aa0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
