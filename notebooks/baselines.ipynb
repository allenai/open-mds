{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization Baselines\n",
    "\n",
    "This notebook computes various simple summarization baselines for the datasets we investigated in the paper.\n",
    "\n",
    "__Note__: if you are running this notebook in colab, uncomment and run the following cell to install the project and its dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"git+https://github.com/allenai/open-mds.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to import the required packages and load some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Optional\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from open_mds import metrics\n",
    "from open_mds.common import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_baselines(dataset, summary_column: str, strategy: str = \"random\", rng_state: int = 42) -> List[str]:\n",
    "    \"\"\"For each example, selects a random reference summary from the dataset to act as the generated summary. If\n",
    "    `strategy` is \"length-matched\", then a random summary of the same or similar length to the true reference\n",
    "    summary is selected.\n",
    "    \"\"\"\n",
    "    random.seed(rng_state)\n",
    "    baseline_summaries = []\n",
    "    summary_lens = None\n",
    "\n",
    "    for i, _ in tqdm(\n",
    "        enumerate(dataset[summary_column]), desc=f\"Summary baseline (strategy={strategy})\", total=len(dataset)\n",
    "    ):\n",
    "        if strategy == \"random\":\n",
    "            baseline_summaries.append(random.choice(dataset[summary_column][:i] + dataset[summary_column][i + 1 :]))\n",
    "        elif strategy == \"length-matched\":\n",
    "            # Only compute once and only compute at all if strategy is similar\n",
    "            if summary_lens is None:\n",
    "                summary_lens = [len(nltk.word_tokenize(summary)) for summary in dataset[summary_column]]\n",
    "            curr_summary_len = summary_lens[i]\n",
    "            other_summary_lens = summary_lens[:i] + summary_lens[i + 1 :]\n",
    "            closest_summary_len = min(other_summary_lens, key=lambda x: abs(x - curr_summary_len))\n",
    "            # Need to insert something at i so that list.index will give us the correct index\n",
    "            other_summary_lens.insert(i, None)\n",
    "            idx = other_summary_lens.index(closest_summary_len)\n",
    "            baseline_summaries.append(dataset[summary_column][idx])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid strategy: {strategy}\")\n",
    "\n",
    "    return baseline_summaries, dataset[summary_column]\n",
    "\n",
    "\n",
    "def document_baselines(\n",
    "    dataset,\n",
    "    text_column: str,\n",
    "    summary_column: str,\n",
    "    strategy: str = \"random\",\n",
    "    lead_only: bool = False,\n",
    "    doc_sep_token: Optional[str] = None,\n",
    "    rng_state: int = 42,\n",
    ") -> List[str]:\n",
    "    \"\"\"For each example, selects a single input document to act as the generated summary. If `strategy` is \"random\",\n",
    "    then the document is selected randomly. If `strategy` is similar, the document with the greatest ROUGE-1 F1\n",
    "    score with the reference summary is selected. If `lead_only` is True, then only the first sentence of the\n",
    "    selected input documents is returned.\n",
    "    \"\"\"\n",
    "    random.seed(rng_state)\n",
    "    baseline_summaries = []\n",
    "    reference_summaries = dataset[summary_column]\n",
    "\n",
    "    # Some datasets, like Multi-XScience, have a nested structure. Handle that here by allowing user to provide\n",
    "    # both column names as \"column_1+column_2\".\n",
    "    document_key = None\n",
    "    if \"+\" in text_column:\n",
    "        text_column, document_key = text_column.split(\"+\")\n",
    "\n",
    "    for documents, summary in tqdm(\n",
    "        zip(dataset[text_column], reference_summaries),\n",
    "        desc=f\"Document baseline (strategy={strategy}, lead_only={lead_only})\",\n",
    "        total=len(dataset),\n",
    "    ):\n",
    "        if doc_sep_token is not None:\n",
    "            documents = documents.split(doc_sep_token)\n",
    "        if document_key is not None:\n",
    "            documents = documents[document_key]\n",
    "\n",
    "        if strategy == \"random\":\n",
    "            baseline_summary = random.choice(documents)\n",
    "        elif strategy == \"oracle\":\n",
    "            scores = metrics.compute_rouge(\n",
    "                predictions=documents, references=[summary] * len(documents), rouge_types=[\"rouge1\"]\n",
    "            )\n",
    "            rouge_1_fmeasure = scores[\"rouge1\"][\"fmeasure\"]\n",
    "            idx = np.argmax(rouge_1_fmeasure)\n",
    "            baseline_summary = documents[idx]\n",
    "        elif strategy == \"all\":\n",
    "            baseline_summary = documents[:]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid strategy: {strategy}\")\n",
    "\n",
    "        if lead_only:\n",
    "            if isinstance(baseline_summary, list):\n",
    "\n",
    "                baseline_summary = [nltk.sent_tokenize(summary)[0] if summary else \"\" for summary in baseline_summary]\n",
    "            else:\n",
    "                baseline_summary = nltk.sent_tokenize(baseline_summary)[0] if baseline_summary else \"\"\n",
    "\n",
    "        if isinstance(baseline_summary, list):\n",
    "            baseline_summary = \". \".join(baseline_summary).strip()\n",
    "\n",
    "        baseline_summaries.append(baseline_summary.strip())\n",
    "\n",
    "    return baseline_summaries, reference_summaries\n",
    "\n",
    "\n",
    "def select_column(dataset, summary_column: str, column: str) -> List[str]:\n",
    "    \"\"\"For each example, selects the text at `column` from the `dataset` to act as the generated summary.\"\"\"\n",
    "\n",
    "    # User could provide a column name that maps to a list, so handle that here\n",
    "    if isinstance(dataset[column][0], list):\n",
    "        column_text = [\". \".join(example) for example in dataset[column]]\n",
    "    else:\n",
    "        column_text = dataset[column]\n",
    "\n",
    "    return column_text, dataset[summary_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run the baselines for each dataset. Note that each dataset may take 10s of minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"multi_news\", split=\"test\")\n",
    "\n",
    "# Random (length-matched) summary\n",
    "predictions, references = summary_baselines(dataset, summary_column=\"summary\", strategy=\"length-matched\")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Random Summary:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle document\n",
    "predictions, references = document_baselines(\n",
    "    dataset,\n",
    "    text_column=\"document\",\n",
    "    summary_column=\"summary\",\n",
    "    strategy=\"oracle\",\n",
    "    lead_only=False,\n",
    "    doc_sep_token=util.DOC_SEP_TOKENS[\"multi_news\"],\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Document:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle lead\n",
    "predictions, references = document_baselines(\n",
    "    dataset,\n",
    "    text_column=\"document\",\n",
    "    summary_column=\"summary\",\n",
    "    strategy=\"oracle\",\n",
    "    lead_only=True,\n",
    "    doc_sep_token=util.DOC_SEP_TOKENS[\"multi_news\"],\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# All lead\n",
    "predictions, references = document_baselines(\n",
    "    dataset,\n",
    "    text_column=\"document\",\n",
    "    summary_column=\"summary\",\n",
    "    strategy=\"all\",\n",
    "    lead_only=True,\n",
    "    doc_sep_token=util.DOC_SEP_TOKENS[\"multi_news\"],\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"All Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ccdv/WCEP-10\", \"list\", split=\"test\")\n",
    "\n",
    "# Random (length-matched) summary\n",
    "predictions, references = summary_baselines(dataset, summary_column=\"summary\", strategy=\"length-matched\")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Random Summary:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle document\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"document\", summary_column=\"summary\", strategy=\"oracle\", lead_only=False\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Document:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle lead\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"document\", summary_column=\"summary\", strategy=\"oracle\", lead_only=True\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# All lead\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"document\", summary_column=\"summary\", strategy=\"all\", lead_only=True\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"All Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"multi_x_science_sum\", split=\"test\")\n",
    "\n",
    "# Random (length-matched) summary\n",
    "predictions, references = summary_baselines(dataset, summary_column=\"related_work\", strategy=\"length-matched\")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Random Summary:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle document\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"ref_abstract+abstract\", summary_column=\"related_work\", strategy=\"oracle\", lead_only=False\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Document:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle lead\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"ref_abstract+abstract\", summary_column=\"related_work\", strategy=\"oracle\", lead_only=True\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# All lead\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"ref_abstract+abstract\", summary_column=\"related_work\", strategy=\"all\", lead_only=True\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"All Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Abstract\n",
    "predictions, references = select_column(dataset, summary_column=\"related_work\", column=\"abstract\")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Abstract:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split=\"validation\")\n",
    "\n",
    "# Random (length-matched) summary\n",
    "predictions, references = summary_baselines(dataset, summary_column=\"target\", strategy=\"length-matched\")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Random Summary:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle document\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"abstract\", summary_column=\"target\", strategy=\"oracle\", lead_only=False\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Document:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle lead\n",
    "# Note, for MS2 and Cochrane we treat the title as the lead, rather than the first sentence of the abstract\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"title\", summary_column=\"target\", strategy=\"oracle\", lead_only=False\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# All lead\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"title\", summary_column=\"target\", strategy=\"all\", lead_only=False\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"All Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Background\n",
    "predictions, references = select_column(dataset, summary_column=\"target\", column=\"background\")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Background:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"allenai/mslr2022\", \"cochrane\", split=\"validation\")\n",
    "\n",
    "# Random (length-matched) summary\n",
    "predictions, references = summary_baselines(dataset, summary_column=\"target\", strategy=\"length-matched\")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Random Summary:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle document\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"abstract\", summary_column=\"target\", strategy=\"oracle\", lead_only=False\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Document:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle lead\n",
    "# Note, for MS2 and Cochrane we treat the title as the lead, rather than the first sentence of the abstract\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"title\", summary_column=\"target\", strategy=\"oracle\", lead_only=False\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# All lead\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"title\", summary_column=\"target\", strategy=\"all\", lead_only=False\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"All Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('open-mds-C-nHY9Y1-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf84cdef7bef6c8a571de4ac52ff8ec016cbb22b7bda79a5961c2e1a10655180"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
