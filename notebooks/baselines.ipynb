{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "\n",
    "This notebook computes various simple baselines for the datasets we evaluate (see the paper for more details).\n",
    "\n",
    "Run the following cells to import the required packages and load some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Optional\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from open_mds import metrics\n",
    "from open_mds.common import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the helper functions which get the predicted and reference summaries for each baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_baselines(dataset, summary_column: str, strategy: str = \"random\", rng_state: int = 42) -> List[str]:\n",
    "    \"\"\"For each example, selects a random reference summary from the dataset to act as the generated summary. If\n",
    "    `strategy` is \"length-matched\", then a random summary of the same or similar length to the true reference\n",
    "    summary is selected.\n",
    "    \"\"\"\n",
    "    random.seed(rng_state)\n",
    "    baseline_summaries = []\n",
    "    summary_lens = None\n",
    "\n",
    "    for i, _ in tqdm(\n",
    "        enumerate(dataset[summary_column]), desc=f\"Summary baseline (strategy={strategy})\", total=len(dataset)\n",
    "    ):\n",
    "        if strategy == \"random\":\n",
    "            baseline_summaries.append(random.choice(dataset[summary_column][:i] + dataset[summary_column][i + 1 :]))\n",
    "        elif strategy == \"length-matched\":\n",
    "            # Only compute once and only compute at all if strategy is similar\n",
    "            if summary_lens is None:\n",
    "                summary_lens = [len(nltk.word_tokenize(summary)) for summary in dataset[summary_column]]\n",
    "            curr_summary_len = summary_lens[i]\n",
    "            other_summary_lens = summary_lens[:i] + summary_lens[i + 1 :]\n",
    "            closest_summary_len = min(other_summary_lens, key=lambda x: abs(x - curr_summary_len))\n",
    "            # Need to insert something at i so that list.index will give us the correct index\n",
    "            other_summary_lens.insert(i, None)\n",
    "            idx = other_summary_lens.index(closest_summary_len)\n",
    "            baseline_summaries.append(dataset[summary_column][idx])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid strategy: {strategy}\")\n",
    "\n",
    "    return baseline_summaries, dataset[summary_column]\n",
    "\n",
    "\n",
    "def document_baselines(\n",
    "    dataset,\n",
    "    text_column: str,\n",
    "    summary_column: str,\n",
    "    strategy: str = \"random\",\n",
    "    lead_only: bool = False,\n",
    "    doc_sep_token: Optional[str] = None,\n",
    "    rng_state: int = 42,\n",
    ") -> List[str]:\n",
    "    \"\"\"For each example, selects a single input document to act as the generated summary. If `strategy` is \"random\",\n",
    "    then the document is selected randomly. If `strategy` is similar, the document with the greatest ROUGE-1 F1\n",
    "    score with the reference summary is selected. If `lead_only` is True, then only the first sentence of the\n",
    "    selected input documents is returned.\n",
    "    \"\"\"\n",
    "    random.seed(rng_state)\n",
    "    baseline_summaries = []\n",
    "    reference_summaries = dataset[summary_column]\n",
    "\n",
    "    # Some datasets, like Multi-XScience, have a nested structure. Handle that here by allowing user to provide\n",
    "    # both column names as \"column_1+column_2\".\n",
    "    document_key = None\n",
    "    if \"+\" in text_column:\n",
    "        text_column, document_key = text_column.split(\"+\")\n",
    "\n",
    "    for documents, summary in tqdm(\n",
    "        zip(dataset[text_column], reference_summaries),\n",
    "        desc=f\"Document baseline (strategy={strategy}, lead_only={lead_only})\",\n",
    "        total=len(dataset),\n",
    "    ):\n",
    "        if doc_sep_token is not None:\n",
    "            documents = documents.split(doc_sep_token)\n",
    "        if document_key is not None:\n",
    "            documents = documents[document_key]\n",
    "\n",
    "        if strategy == \"random\":\n",
    "            baseline_summary = random.choice(documents)\n",
    "        elif strategy == \"oracle\":\n",
    "            scores = metrics.compute_rouge(\n",
    "                predictions=documents, references=[summary] * len(documents), rouge_types=[\"rouge1\"]\n",
    "            )\n",
    "            rouge_1_fmeasure = scores[\"rouge1\"][\"fmeasure\"]\n",
    "            idx = np.argmax(rouge_1_fmeasure)\n",
    "            baseline_summary = documents[idx]\n",
    "        elif strategy == \"all\":\n",
    "            baseline_summary = documents[:]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid strategy: {strategy}\")\n",
    "\n",
    "        if lead_only:\n",
    "            if isinstance(baseline_summary, list):\n",
    "\n",
    "                baseline_summary = [nltk.sent_tokenize(summary)[0] if summary else \"\" for summary in baseline_summary]\n",
    "            else:\n",
    "                baseline_summary = nltk.sent_tokenize(baseline_summary)[0] if baseline_summary else \"\"\n",
    "\n",
    "        if isinstance(baseline_summary, list):\n",
    "            baseline_summary = \". \".join(baseline_summary).strip()\n",
    "\n",
    "        baseline_summaries.append(baseline_summary.strip())\n",
    "\n",
    "    return baseline_summaries, reference_summaries\n",
    "\n",
    "\n",
    "def select_column(dataset, summary_column: str, column: str) -> List[str]:\n",
    "    \"\"\"For each example, selects the text at `column` from the `dataset` to act as the generated summary.\"\"\"\n",
    "\n",
    "    # User could provide a column name that maps to a list, so handle that here\n",
    "    if isinstance(dataset[column][0], list):\n",
    "        column_text = [\". \".join(example) for example in dataset[column]]\n",
    "    else:\n",
    "        column_text = dataset[column]\n",
    "\n",
    "    return column_text, dataset[summary_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run the baselines for each dataset. Note that each dataset may take 10s of minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"multi_news\", split=\"test\")\n",
    "\n",
    "# Random (length-matched) summary\n",
    "predictions, references = summary_baselines(dataset, summary_column=\"summary\", strategy=\"length-matched\")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Random Summary:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle document\n",
    "predictions, references = document_baselines(\n",
    "    dataset,\n",
    "    text_column=\"document\",\n",
    "    summary_column=\"summary\",\n",
    "    strategy=\"oracle\",\n",
    "    lead_only=False,\n",
    "    doc_sep_token=util.DOC_SEP_TOKENS[\"multi_news\"],\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Document:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle lead\n",
    "predictions, references = document_baselines(\n",
    "    dataset,\n",
    "    text_column=\"document\",\n",
    "    summary_column=\"summary\",\n",
    "    strategy=\"oracle\",\n",
    "    lead_only=True,\n",
    "    doc_sep_token=util.DOC_SEP_TOKENS[\"multi_news\"],\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# All lead\n",
    "predictions, references = document_baselines(\n",
    "    dataset,\n",
    "    text_column=\"document\",\n",
    "    summary_column=\"summary\",\n",
    "    strategy=\"all\",\n",
    "    lead_only=True,\n",
    "    doc_sep_token=util.DOC_SEP_TOKENS[\"multi_news\"],\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"All Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ccdv/WCEP-10\", \"list\", split=\"test\")\n",
    "\n",
    "# Random (length-matched) summary\n",
    "predictions, references = summary_baselines(dataset, summary_column=\"summary\", strategy=\"length-matched\")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Random Summary:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle document\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"document\", summary_column=\"summary\", strategy=\"oracle\", lead_only=False\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Document:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle lead\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"document\", summary_column=\"summary\", strategy=\"oracle\", lead_only=True\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# All lead\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"document\", summary_column=\"summary\", strategy=\"all\", lead_only=True\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"All Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"multi_x_science_sum\", split=\"test\")\n",
    "\n",
    "# Random (length-matched) summary\n",
    "predictions, references = summary_baselines(dataset, summary_column=\"related_work\", strategy=\"length-matched\")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Random Summary:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle document\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"ref_abstract+abstract\", summary_column=\"related_work\", strategy=\"oracle\", lead_only=False\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Document:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle lead\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"ref_abstract+abstract\", summary_column=\"related_work\", strategy=\"oracle\", lead_only=True\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# All lead\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"ref_abstract+abstract\", summary_column=\"related_work\", strategy=\"all\", lead_only=True\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"All Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Abstract\n",
    "predictions, references = select_column(dataset, summary_column=\"related_work\", column=\"abstract\")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Abstract:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split=\"validation\")\n",
    "\n",
    "# Random (length-matched) summary\n",
    "predictions, references = summary_baselines(dataset, summary_column=\"target\", strategy=\"length-matched\")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Random Summary:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle document\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"abstract\", summary_column=\"target\", strategy=\"oracle\", lead_only=False\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Document:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle lead\n",
    "# Note, for MS2 and Cochrane we treat the title as the lead, rather than the first sentence of the abstract\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"title\", summary_column=\"target\", strategy=\"oracle\", lead_only=False\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# All lead\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"title\", summary_column=\"target\", strategy=\"all\", lead_only=False\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"All Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Background\n",
    "predictions, references = select_column(dataset, summary_column=\"target\", column=\"background\")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Background:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"allenai/mslr2022\", \"cochrane\", split=\"validation\")\n",
    "\n",
    "# Random (length-matched) summary\n",
    "predictions, references = summary_baselines(dataset, summary_column=\"target\", strategy=\"length-matched\")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Random Summary:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle document\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"abstract\", summary_column=\"target\", strategy=\"oracle\", lead_only=False\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Document:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# Oracle lead\n",
    "# Note, for MS2 and Cochrane we treat the title as the lead, rather than the first sentence of the abstract\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"title\", summary_column=\"target\", strategy=\"oracle\", lead_only=False\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"Oracle Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))\n",
    "\n",
    "# All lead\n",
    "predictions, references = document_baselines(\n",
    "    dataset, text_column=\"title\", summary_column=\"target\", strategy=\"all\", lead_only=False\n",
    ")\n",
    "rouge_results = metrics.compute_rouge(predictions=predictions, references=references)\n",
    "print(\"All Lead:\", round(rouge_results[\"rouge_avg_fmeasure_mean\"], 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('open-mds-C-nHY9Y1-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf84cdef7bef6c8a571de4ac52ff8ec016cbb22b7bda79a5961c2e1a10655180"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
