{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Evaluation\n",
    "\n",
    "This notebook organizes the collection of data and the analysis for the human evaluation.\n",
    "\n",
    "__Note__: if you are running this notebook in colab, uncomment and run the following cell to install the project and its dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"git+https://github.com/allenai/open-mds.git\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to import the required packages and load some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from open_mds.common import util\n",
    "\n",
    "# Set random seed of python module for reproducibility\n",
    "random_seed = 42\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Data Collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the parameters of the human evaluation. Please provide the directory to the raw experiemental results (`data_dir`), alongside the model (`model_name_or_path`), `retriever` and `top_k_strategy` to evaluate. You should also select the `num_samples` to draw for human annotation.\n",
    "\n",
    "The following cells will collect the data for human evaluation and save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O paths\n",
    "data_dir = \"../output/results/multinews/\"\n",
    "output_dir = \"../output/human_eval/\"\n",
    "\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "examples_path = Path(output_dir) / \"examples.tsv\"\n",
    "model_key_path = Path(output_dir) / \"model_key.tsv\"\n",
    "annotations_path = Path(output_dir) / \"annotations\"\n",
    "\n",
    "# Parameters of the human evaluation\n",
    "model_name_or_path = \"allenai/PRIMERA-multinews\"\n",
    "retriever = \"sparse\"\n",
    "top_k_strategy = \"mean\"\n",
    "num_samples = 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the raw data, subsetting it given the provided parameters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some datasets have blind test splits, and so we evaluate on the validation set\n",
    "# HuggingFace assigns a different prefix to the keys in the output json, so set that here\n",
    "metric_key_prefix = \"eval\" if Path(data_dir).name in {\"ms2\", \"cochrane\"} else \"predict\"\n",
    "\n",
    "# The metrics we want to record results for\n",
    "metric_columns = [\n",
    "    f\"{metric_key_prefix}_rouge_avg_fmeasure\",\n",
    "    f\"{metric_key_prefix}_bertscore_f1\",\n",
    "]\n",
    "\n",
    "# Load the results as dataframes\n",
    "baseline_df, retrieval_df = util.load_results_dicts(\n",
    "    data_dir=data_dir,\n",
    "    metric_columns=metric_columns,\n",
    "    metric_key_prefix=metric_key_prefix,\n",
    "    # Only retain data that pertains to the retrieval experiments\n",
    "    load_perturbation_results=False,\n",
    "    load_training_results=False,\n",
    ")\n",
    "\n",
    "# Filter to just the data we want to compare\n",
    "baseline_df = baseline_df[baseline_df.model_name_or_path == model_name_or_path]\n",
    "retrieval_df = retrieval_df.loc[\n",
    "    (retrieval_df.model_name_or_path == model_name_or_path)\n",
    "    & (retrieval_df[f\"{metric_key_prefix}_retriever\"] == retriever)\n",
    "    & (retrieval_df[f\"{metric_key_prefix}_top_k_strategy\"] == top_k_strategy)\n",
    "]\n",
    "\n",
    "# Reset the index so that it is numbered from 0...N like the baseline dataframe\n",
    "retrieval_df = retrieval_df.reset_index()\n",
    "\n",
    "# Sanity check that baseline and retrieval dfs have the same index and correspond to the same examples\n",
    "assert baseline_df.index.equals(retrieval_df.index)\n",
    "assert baseline_df[f\"{metric_key_prefix}_labels\"].equals(retrieval_df[f\"{metric_key_prefix}_labels\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, randomly sample the examples for the human evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that repeated runs of this cell produce the same results\n",
    "rng = random.Random(random_seed)\n",
    "example_ids = rng.sample(retrieval_df.index.tolist(), num_samples)\n",
    "\n",
    "sampled_baseline = baseline_df.loc[example_ids, :]\n",
    "sampled_retrieval = retrieval_df.loc[example_ids, :]\n",
    "\n",
    "# Sanity check that the sampled instances correspond to the same examples\n",
    "assert (\n",
    "    sampled_baseline[f\"{metric_key_prefix}_labels\"].tolist()\n",
    "    == sampled_retrieval[f\"{metric_key_prefix}_labels\"].tolist()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, collect the data we need for human evaluation in the correct format and save it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_summaries = sampled_baseline[f\"{metric_key_prefix}_labels\"].tolist()\n",
    "baseline_summaries = sampled_baseline[f\"{metric_key_prefix}_preds\"].tolist()\n",
    "retrieval_summaries = sampled_retrieval[f\"{metric_key_prefix}_preds\"].tolist()\n",
    "\n",
    "target_summaries = [util.sanitize_text(summary) for summary in target_summaries]\n",
    "baseline_summaries = [util.sanitize_text(summary) for summary in baseline_summaries]\n",
    "retrieval_summaries = [util.sanitize_text(summary) for summary in retrieval_summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that repeated runs of this cell produce the same results\n",
    "rng = random.Random(random_seed)\n",
    "model_orders = rng.choices([True, False], k=num_samples)\n",
    "\n",
    "with open(examples_path, \"w\") as examples:\n",
    "    with open(model_key_path, \"w\") as model_key:\n",
    "\n",
    "        # Write the headers\n",
    "        examples.write(\"example_id\\ttarget_summary\\tmodel_1_summary\\tmodel_2_summary\\n\")\n",
    "        model_key.write(\"example_id\\tmodel_1\\tmodel_2\\n\")\n",
    "\n",
    "        for example_id, target_summary, baseline_summary, retrieval_summary, baseline_first in zip(\n",
    "            example_ids, target_summaries, baseline_summaries, retrieval_summaries, model_orders\n",
    "        ):\n",
    "            examples.write(f\"{example_id}\\t{target_summary}\\t\")\n",
    "            model_key.write(f\"{example_id}\\t\")\n",
    "\n",
    "            # Randomly order the model summaries\n",
    "            if baseline_first:\n",
    "                examples.write(f\"{baseline_summary}\\t{retrieval_summary}\\n\")\n",
    "                model_key.write(\"baseline\\tretrieval\\n\")\n",
    "            else:\n",
    "                examples.write(f\"{retrieval_summary}\\t{baseline_summary}\\n\")\n",
    "                model_key.write(\"retrieval\\tbaseline\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The following cells will load the human annotations and perform the analysis. The `tsv` files containing the annotations from each human annotator should be in `output_dir / \"annotations\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model key\n",
    "model_key_df = pd.read_csv(model_key_path, sep=\"\\t\")\n",
    "\n",
    "# Load the human annotations\n",
    "annotator_dfs = []\n",
    "annotations_path = Path(output_dir) / \"annotations\"\n",
    "for annotator_file in annotations_path.glob(\"*.tsv\"):\n",
    "    # The first three lines are instructions, and the last column is annotator comments, skip\n",
    "    annotator_dfs.append(pd.read_csv(annotator_file, sep=\"\\t\", skiprows=3, usecols=list(range(6))))\n",
    "annotator_df = pd.concat(annotator_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE = \"baseline\"\n",
    "# The three possible votes for each facet\n",
    "A_VOTE = \"A\"\n",
    "B_VOTE = \"B\"\n",
    "NEITHER = \"Neither\"\n",
    "# The two facets we're evaluating\n",
    "FACETS = [\"coverage\", \"informativeness\"]\n",
    "\n",
    "for facet in FACETS:\n",
    "\n",
    "    baseline_votes, total_votes = 0, 0\n",
    "\n",
    "    for i, ann in enumerate(annotator_df[facet].to_list()):\n",
    "        row_idx = annotator_df.index[i]\n",
    "\n",
    "        # Make sure differences aren't due to whitespace or case\n",
    "        ann = ann.strip().title()\n",
    "\n",
    "        # Unlikely, but catch unknown annotations\n",
    "        if ann not in [A_VOTE, B_VOTE, NEITHER]:\n",
    "            raise ValueError(f'Unknown annotation (\"{ann}\") in row {i} of the human evaluation results')\n",
    "\n",
    "        # Skip ties\n",
    "        if ann == \"Neither\":\n",
    "            continue\n",
    "\n",
    "        # Collect the votes for the baseline, resolving which model is the baseline based on the model key\n",
    "        col_name = \"model_1\" if ann == A_VOTE else \"model_2\"\n",
    "        baseline_votes += int(model_key_df.loc[row_idx, col_name].strip().lower() == BASELINE)\n",
    "        total_votes += 1\n",
    "\n",
    "    print(f'Results for facet: \"{facet}\"')\n",
    "    print(scipy.stats.binomtest(baseline_votes, total_votes, p=0.5, alternative=\"two-sided\"))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-mds-KCQg3FFx-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1de8a9ac52ca6a8e43f03f98931f021acf0adbba0952f200363bc18db767aa0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
